{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and libraries needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the following using Rapids functions cudf, and cuml --> it enables us run on GPUs which is far faster\n",
    "df2 = pd.read_csv(\"data/diabetes.csv\").drop('PatientID', axis=1)\n",
    "\n",
    "def train_model(params, test_size = 0.3, registered_model_name= None):\n",
    "    #max_depth, max_features, n_estimators = params\n",
    "    \n",
    "    # Split between features and label\n",
    "    X = df2.drop([\"Diabetic\"], axis = 1)\n",
    "    y = df2[\"Diabetic\"]\n",
    "    \n",
    "    # define training and test set based on split in function\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 123)\n",
    "    \n",
    "    # model and fit\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # log tags, and metrics\n",
    "    if test_size > 0.0:\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        \n",
    "    else:\n",
    "        acc = np.nan\n",
    "        prec = np.nan\n",
    "        rec = np.nan\n",
    "    \n",
    "    # Since fmin (hyperopt) minimizes loss, we want to maximise acc -- which is the reverse --> return this loss function\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': -0.9376666666666666, 'status': 'ok'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"max_features\": 0.7, \"n_estimators\": 100, \"max_depth\" : 10}\n",
    "train_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.64trial/s, best loss: -0.9256666666666666]\n"
     ]
    }
   ],
   "source": [
    "# HyperOpt\n",
    "# Configure search space\n",
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "   # 'max_features': hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "    'max_features': hp.uniform(\"max_features\", 0.0, 1.0),\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 200, 50))\n",
    "}\n",
    "\n",
    "algo = tpe.suggest\n",
    "\n",
    "# Define spark trials object - for Databricks, this keeps track of all your trials and integrates with databricks UI\n",
    "trials = Trials()\n",
    "\n",
    "# fmin returns best parameters \n",
    "best = fmin(\n",
    "            fn = train_model,\n",
    "            space = search_space,\n",
    "            algo = algo,\n",
    "            trials = trials,\n",
    "            max_evals = 2,\n",
    "        )\n",
    "\n",
    "\n",
    "best_params = {\"max_features\": best[\"max_features\"], \"n_estimators\": int(best[\"n_estimators\"]), \"max_depth\" : int(best[\"max_depth\"])}\n",
    "# train_model(best_params, test_size = 0.0001, registered_model_name=\"Diabetes_Prediction\") # registered_model_name kept giving an error --> investigate\n",
    "# RestException: INVALID_PARAMETER_VALUE:  Model registry functionality is unavailable; got unsupported URI './mlruns' for model registry data storage. Supported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.\n",
    "\n",
    "X = df2.drop([\"Diabetic\"], axis = 1).values\n",
    "y = df2[\"Diabetic\"].values\n",
    "\n",
    "# fit to all data\n",
    "best_model = RandomForestClassifier(**best_params).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_no_mlflow/model.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('output_no_mlflow'):\n",
    "    os.makedirs('output_no_mlflow')\n",
    "dump(best_model, \"output_no_mlflow/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Counter({0: 6888, 1: 3112})\n",
      "[0]\n",
      "Counter({0: 6888, 1: 3112})\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import pickle\n",
    "import collections\n",
    "test_data = [0, 171, 80, 34, 23, 43.509726, 1.213191, 21]\n",
    "test_data_reshaped = np.array(test_data).reshape(1,-1)\n",
    "\n",
    "prediction_1 = best_model.predict(test_data_reshaped)\n",
    "print(prediction_1)\n",
    "\n",
    "prediction_1_batch = best_model.predict(X)\n",
    "print(collections.Counter(prediction_1_batch))\n",
    "\n",
    "loaded_model = load(\"output_no_mlflow/model.pkl\")\n",
    "prediction_2 = loaded_model.predict(test_data_reshaped)\n",
    "print(prediction_2)\n",
    "\n",
    "prediction_2_batch = loaded_model.predict(X)\n",
    "print(collections.Counter(prediction_2_batch))\n",
    "\n",
    "# Pickle.Load gave this error \"UnpicklingError: invalid load key, '\\x00'.\"\n",
    "# loaded_model_pkl = pickle.load(open(\"output_no_mlflow/model.pkl\", \"rb\"))\n",
    "# prediction_3 = loaded_model_pkl.predict(test_data_reshaped)\n",
    "# print(prediction_3)\n",
    "\n",
    "test_data_no_ID = [0, 171, 80, 34, 23, 43.509726, 1.213191, 21]\n",
    "test_data_reshaped_noID = np.array(test_data_no_ID).reshape(1,-1)\n",
    "prediction_4 = loaded_model.predict(test_data_reshaped_noID)\n",
    "print(prediction_4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1a2a7e0bb8c2eb4bb83d0db03e33cfe6c525724c28ef03edefe88bb35831af0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
